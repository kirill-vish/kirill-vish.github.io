<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> | Kirill Vishniakov</title> <meta name="author" content="Kirill Vishniakov"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/website-logo.png?018886d62ef98981b5bd90a9a176f244"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kirill-vish.github.io/beyond-imagenet-accuracy/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kirill </span>Vishniakov</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"></h1> <p class="post-description"></p> </header> <article> <div style="text-align: center;"> <h2 style="font-weight: bold;">ConvNet vs Transformer, Supervised vs CLIP:</h2> <h2 style="font-weight: bold;">Beyond ImageNet Accuracy</h2> <h4> <strong style="margin-right: 30px;"><a href="https://kirill-vish.github.io/" target="_blank" rel="noopener noreferrer">Kirill Vishniakov¹</a></strong> <strong style="margin-right: 30px;"><a href="https://zhiqiangshen.com/" target="_blank" rel="noopener noreferrer">Zhiqiang Shen¹</a></strong> <strong><a href="https://liuzhuang13.github.io/" target="_blank" rel="noopener noreferrer">Zhuang Liu²</a></strong> </h4> <div style="display: flex; justify-content: center; align-items: center; gap: 40px;"> <div style="text-align: center; position: relative; max-width: 25%;"> <sup style="position: absolute; left: -0.5em; top: 1.4em; padding-right: 3px; font-size: 18px; font-weight: normal;">1</sup> <a href="https://mbzuai.ac.ae/" rel="external nofollow noopener" target="_blank"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/mbzuai_logo.jpg" class="img-fluid rounded" width="auto" height="auto" title="MBZUAI" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </a> </div> <div style="text-align: center; position: relative; max-width: 25%;"> <sup style="position: absolute; left: -0.5em; top: -0.25em; padding-right: 3px; font-size: 18px; font-weight: normal;">2</sup> <a href="https://ai.meta.com/" rel="external nofollow noopener" target="_blank"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/meta_logo.png" class="img-fluid rounded" width="auto" height="auto" title="META AI Research" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </a> </div> </div> </div> <div class="buttons-container" style="text-align: center; margin-bottom: 20px;"> <a href="https://arxiv.org/pdf/2311.09215.pdf" target="_blank" rel="noopener noreferrer" class="btn"> Paper </a> <a href="https://github.com/kirill-vish/Beyond-INet" target="_blank" rel="noopener noreferrer" class="btn"> Code </a> </div> <style>.btn{border:1px solid black;border-radius:15px;background-color:white;padding:5px 10px;display:inline-block;text-decoration:none;color:black;font-size:14px;margin:0 5px;transition:background-color .3s,color .3s}.buttons-container{margin-bottom:30px;margin-top:-15px}.btn:hover{background-color:black;color:white}</style> <style>body{font-weight:normal}</style> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <h3 style="text-align: center;"><strong>Abstract</strong></h3> <p> Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. </p> </div> </div> <hr> <div class="col-sm mt-3 mt-md-0 d-flex justify-content-center"> <div style="width: 50%;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/method.png" class="img-fluid rounded" width="auto" height="auto" title="Fig 1: Overview of the Method" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Models are often compared only by their ImageNet accuracy, ignoring many other aspects of their behaviors. Our study shows models with similar ImageNet performance can have vastly different properties.</p> <hr> <h4 style="font-weight: bold;">Models</h4> <p>To examine the impact of architecture and training objective on model performance, we compare Vision Transformer (ViT) with ConvNeXt, both modern architectures with comparable ImageNet-1K validation accuracies and computational requirements. Our study contrasts supervised models, represented by DeiT3-Base/16 and ConvNeXt-Base, and vision encoders of CLIP-based models from OpenCLIP.</p> <div class="col-sm mt-3 mt-md-0 d-flex justify-content-center"> <div style="width: 100%;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/models.jpg" class="img-fluid rounded" width="auto" height="auto" title="Fig 1: Overview of the Method" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h4 style="font-weight: bold;">Property Analysis</h4> <p>Our analysis is designed to investigate model behaviors that can be evaluated without the need for further training or finetuning. This approach is particularly relevant for practitioners with limited computational resources, who often depend on pretrained models. While we recognize the value of downstream tasks like object detection, our focus is on properties that offer insights with minimal computational demands and reflect behaviors important for real-world applications.</p> <h5 style="font-weight: bold;">Model Mistakes</h5> <p>ImageNet-X is a dataset that extends ImageNet-1K with detailed human annotations for 16 factors of variation, enabling an in-depth analysis of model mistakes in image classification. It employs an error ratio metric (lower is better) to quantify model performance on specific factors relative to overall accuracy, allowing for a nuanced analysis of model mistakes. ImageNet-X results illustrate that:</p> <ol> <li> CLIP models make fewer mistakes relative to their ImageNet accuracy than supervised. </li> <li> All models suffer mostly from complex factors like occlusion. </li> <li> Texture is the most challenging factor for all models. </li> </ol> <div class="col-sm mt-3 mt-md-0 d-flex justify-content-center"> <div style="width: 100%;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/imagenetx_clip-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Model Mistakes in ImageNet-X with CLIP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/imagenetx_sup-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Model Mistakes in ImageNet-X with Supervised Learning" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h5 style="font-weight: bold;">Shape / Texture Bias</h5> <p>Shape-texture bias examines if models rely on brittle texture shortcuts rather than high-level shape cues. This bias can be studied using cue-conflict images that combine shapes and textures from different classes. This approach helps to understand how much the model’s decisions are based on shape compared to texture. We evaluate shape-texture bias on cue-conflict dataset, observing that CLIP models have smaller texture bias than supervised and that ViT models have higher shape bias than ConvNets.</p> <div class="d-flex justify-content-center"> <div style="width: 50%; padding-right: 10px;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/shape_texture_clip-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Shape/Texture Bias with CLIP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div style="width: 50%; padding-left: 10px;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/shape_texture_sup-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Shape/Texture Bias with Supervised Learning" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h5 style="font-weight: bold;">Model Calibration</h5> <p>Calibration quantifies whether a model’s predicted confidence aligns with its actual accuracy. This is assessed through metrics such as Expected Calibration Error (ECE) and visual tools including reliability diagrams and confidence histograms. We evaluate calibration on ImageNet-1K and ImageNet-R, dividing predictions into 15 bins. In our experiments we observe the following:</p> <ol> <li> CLIP models are overconfident and supervised models are slightly underconfident. </li> <li> Supervised ConvNeXt is better calibrated than supervised ViT. </li> </ol> <div class="col-sm mt-3 mt-md-0 d-flex justify-content-center"> <div style="width: 100%;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/ImageNet-1K_calibration_subplot-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Model Calibration on ImageNet-1K" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/gray_line-1.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/ImageNet-R_calibration_subplot-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Model Calibration on ImageNet-R" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h5 style="font-weight: bold;">Robustness &amp; Transferability</h5> <p>Model robustness and transferability are essential for adapting to data distribution shifts and new tasks. We evaluated robustness using various ImageNet variants and found that while ViT and ConvNeXt models have comparable average performances, supervised models generally outperformed CLIP on robustness except for ImageNet-R and ImageNet-Sketch. In terms of transferability, evaluated using the VTAB benchmark with 19 datasets, supervised ConvNeXt was superior to ViT almost matching the performance of CLIP models.</p> <div class="d-flex justify-content-center"> <div style="width: 50%; padding-right: 10px;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/vtab_robust_clip_abs.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Robustness and Transferability with CLIP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div style="width: 50%; padding-left: 10px;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/vtab_robust_sup_abs.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Robustness and Transferability with Supervised Learning" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h5 style="font-weight: bold;">Synthetic Data</h5> <p>Synthetic datasets like PUG-ImageNet allow precise control over factors like camera angles and textures, emerging as a promising research avenue, so we analyze model performance on synthetic data. PUG-ImageNet contains photorealistic ImageNet images with systematic variation in factors like pose and lighting, with performance measured by absolute top-1 accuracy. We provide results for different factors in PUG-ImageNet, finding that ConvNeXt outperforms ViT on nearly all factors. This suggests ConvNeXt is better than ViT on synthetic data, with a smaller gap for CLIP models which have lower accuracy than supervised models, likely related to inferior original ImageNet accuracy.</p> <div class="col-sm mt-3 mt-md-0 d-flex justify-content-center"> <div style="width: 100%;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/pug_imagenet_clip-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Synthetic Data with CLIP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/pug_imagenet_sup-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Synthetic Data with Supervised Learning" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h5 style="font-weight: bold;">Transformation Invariance</h5> <p>Transformation invariance refers to a model’s ability to produce consistent representations unaffected by input transformations that preserve semantic meaning, such as scaling or shifting. This property enables models to generalize well across different but semantically similar inputs. Our methodology involves resizing images for scale invariance, shifting crops for positional invariance, and adjusting resolution with interpolated positional embeddings for the ViT model. We assess invariance to scale, shift, and resolution on ImageNet-1K by varying crop scale/location and image resolution. ConvNeXt outperforms ViT under supervised training. Overall, models are more robust to shift than scale/resolution transforms. For applications requiring high robustness to scale, shift and resolution, our results indicate supervised ConvNeXt could be the best choice.</p> <div class="col-sm mt-3 mt-md-0 d-flex justify-content-center"> <div style="width: 100%;"> <figure> <picture> <img src="/assets/img/beyond-imagenet-accuracy/invariance_all-1.png" class="img-fluid rounded" width="auto" height="auto" title="Fig: Transformation Invariance" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h5 style="font-weight: bold;">Conclusions</h5> <p> We found that each model can have its own distinct strengths. This suggests that model selection should depend on the target use cases, as standard performance metrics may overlook key task-specific nuances. In addition, many existing benchmarks are derived from ImageNet which biases the evaluation. Developing new benchmarks with different data distributions will be crucial for evaluating models in a more real-world representative context. </p> <p> <strong>ConvNet vs Transformer</strong><br> </p> <ol> <li> We found the superior performance of supervised ConvNeXt over supervised ViT on many benchmarks: it is better calibrated, more invariant to data transformations, and demonstrates better transferability and robustness. </li> <li> ConvNeXt outperforms ViT on synthetic data.</li> <li> ViT has a higher shape bias. </li> </ol> <p> <strong>Supervised vs CLIP</strong> </p> <ol> <li> Despite CLIP models being better at transferability, supervised ConvNeXt shows a competitive performance on this task. This showcases the potential of supervised models. </li> <li> Supervised models are better at robustness benchmarks, likely because these are ImageNet variants. </li> <li> CLIP models have a higher shape bias and make fewer classification mistakes relative to their ImageNet accuracy.</li> </ol> <h5 style="font-weight: bold;">Contact</h5> <p> <a href="mailto:ki.vishniakov@gmail.com">ki [dot] vishniakov [at] gmail [dot] com</a> </p> <h5 style="font-weight: bold;">Citation</h5> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{vishniakov2023convnet,
      title={ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy}, 
      author={Kirill Vishniakov and Zhiqiang Shen and Zhuang Liu},
      year={2023},
      eprint={2311.09215},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</code></pre></div></div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>